{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "JRq7A-g_zpFe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS_s2gy-r6yD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch import bfloat16\n",
        "import re\n",
        "import bitsandbytes\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from nltk.translate.gleu_score import sentence_gleu\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('/content/tu_impersonal_ejemplos_chatGPT.xlsx')"
      ],
      "metadata": {
        "id": "mlGcWeXUxqNl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data.rename(columns={\n",
        "    'Frase Incorrecta (Tú Impersonal)': 'input',\n",
        "    'Frase Correcta (Se Impersonal)': 'target'\n",
        "})\n",
        "\n",
        "data['input'] = data['input'].str.lower()\n",
        "data['target'] = data['target'].str.lower()\n",
        "\n",
        "def eliminar_espacios_innecesarios(texto):\n",
        "    texto = texto.strip()\n",
        "    texto = re.sub(r'\\s{2,}', ' ', texto)\n",
        "    return texto\n",
        "\n",
        "def anadir_punto_final(texto):\n",
        "    texto = texto.strip()\n",
        "    if not texto.endswith('.'):\n",
        "        texto += '.'\n",
        "    return texto\n",
        "\n",
        "data['input'] = data['input'].apply(anadir_punto_final)\n",
        "data['target'] = data['target'].apply(anadir_punto_final)\n",
        "data['input'] = data['input'].apply(eliminar_espacios_innecesarios)\n",
        "data['target'] = data['target'].apply(eliminar_espacios_innecesarios)"
      ],
      "metadata": {
        "id": "Hsf7ZfNi1FPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "nf4_config = transformers.BitsAndBytesConfig(\n",
        " load_in_4bit=True,\n",
        " bnb_4bit_quant_type='nf4',\n",
        " bnb_4bit_use_double_quant=True,\n",
        " bnb_4bit_compute_dtype=bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "4P_HVGffs4LT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "BvpEE3xgs555"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
        "\n",
        "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        " model_id,\n",
        " trust_remote_code=True,\n",
        " quantization_config=nf4_config,\n",
        " device_map='auto',\n",
        ")\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "vS8YFNztvAa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generator = transformers.pipeline(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    task='text-generation',\n",
        "    device_map=\"auto\",\n",
        "    max_new_tokens = 256,\n",
        ")"
      ],
      "metadata": {
        "id": "eVyIuvn0vLOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Eres un experto en corrección de textos en español. Tu tarea es transformar enunciados en segunda persona del singular (\"tú\") en formulaciones impersonales con \"se\", adecuando el tiempo verbal, número y persona al contexto, y manteniendo el significado original.\n",
        "<</SYS>>\n",
        "\n",
        "Ejemplos:\n",
        "- Original: Si no estudias, no apruebas los exámenes.\n",
        "- Corregido: Si no se estudia, no se aprueban los exámenes.\n",
        "- Original: Durante el proceso de análisis de datos, tú organizas la información en categorías.\n",
        "- Corregido: Durante el proceso de análisis de datos, se organiza la información en categorías.\n",
        "\n",
        "Instrucción:\n",
        "Corrige el siguiente texto siguiendo estas indicaciones y devuelve únicamente el texto reformulado:\n",
        "\n",
        "[DOCUMENTO]\n",
        "\n",
        "Devuelve solo el texto corregido.\n",
        "[/INST]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KctQ32bXtAHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultados = []\n",
        "\n",
        "for idx, row in data.iterrows():\n",
        "    frase_original = row['input']\n",
        "    referencia = row['target']\n",
        "\n",
        "    prompt_actual = prompt.replace(\"[DOCUMENTO]\", frase_original)\n",
        "\n",
        "    generado = generator(\n",
        "        prompt_actual,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    salida_modelo = generado[0]['generated_text'].split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    ref_tokens = referencia.split()\n",
        "    gen_tokens = salida_modelo.split()\n",
        "    gleu = sentence_gleu([ref_tokens], gen_tokens)\n",
        "\n",
        "    resultados.append({\n",
        "        \"original\": frase_original,\n",
        "        \"generada\": salida_modelo,\n",
        "        \"referencia\": referencia,\n",
        "        \"gleu\": gleu\n",
        "    })\n",
        "\n",
        "    print(f\"Original: {frase_original}\")\n",
        "    print(f\"Generada: {salida_modelo}\")\n",
        "    print(f\"Referencia: {referencia}\")\n",
        "    print(f\"GLEU: {gleu:.4f}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "df_resultados = pd.DataFrame(resultados)\n",
        "\n",
        "gleu_medio = df_resultados['gleu'].mean()\n",
        "print(f\"GLEU medio de todas las frases: {gleu_medio:.4f}\")"
      ],
      "metadata": {
        "id": "9dxh9pPf1CuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_feedback = \"\"\"\n",
        "<s>[INST] <<SYS>>\n",
        "Eres experto en retroalimentación lingüística en español. Tu tarea es analizar una frase recibida con error y su corrección. Explica el error y por qué, en contextos formales o escritos, conviene usar formas impersonales con \"se\" y verbo en tercera persona. Indica la importancia de ajustar tiempo, número y persona para mantener la concordancia. Da un feedback claro y concreto, sin repetir la corrección literal.\n",
        "<</SYS>>\n",
        "\n",
        "Ejemplos:\n",
        "- Original: Si no estudias, no apruebas los exámenes.\n",
        "- Corregido: Si no se estudia, no se aprueban los exámenes.\n",
        "- Feedback: En contextos formales se recomienda evitar el \"tú\" para mantener impersonalidad y objetividad. Por eso se usa la construcción con \"se\" y el verbo en tercera persona plural, concordando con \"exámenes\".\n",
        "\n",
        "Instrucción:\n",
        "A continuación, se muestra la frase con error y su corrección. Proporciona un feedback específico que explique el error y oriente sobre cómo evitarlo:\n",
        "\n",
        "Original: [FRASE_CON_ERROR]\n",
        "Corregido: [FRASE_CORREGIDA]\n",
        "[/INST]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "6Pz_i-21OQuU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resultados = []\n",
        "\n",
        "for idx, row in data.iterrows():\n",
        "    frase_original = row['input']\n",
        "    referencia = row['target']\n",
        "\n",
        "    prompt_actual = (\n",
        "        prompt_feedback\n",
        "        .replace(\"[FRASE_CON_ERROR]\", frase_original)\n",
        "        .replace(\"[FRASE_CORREGIDA]\", referencia)\n",
        "    )\n",
        "\n",
        "    generado = generator(\n",
        "        prompt_actual,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=False\n",
        "    )\n",
        "\n",
        "    salida_modelo = generado[0]['generated_text'].split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    resultados.append({\n",
        "        \"original\": frase_original,\n",
        "        \"corregida\": referencia,\n",
        "        \"feedback\": salida_modelo,\n",
        "    })\n",
        "\n",
        "    print(f\"Original (con error): {frase_original}\")\n",
        "    print(f\"Corregida: {referencia}\")\n",
        "    print(f\"Feedback: {salida_modelo}\")\n",
        "    print(\"-\" * 60)"
      ],
      "metadata": {
        "id": "4o3sCCyDOT97"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
